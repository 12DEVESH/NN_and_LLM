{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementing Skip-gram Word Embeddings in PyTorch**\n",
        "\n",
        "Arguably the most crucial step in NLP is to convert words and sentences into a form that neural networks can work with - a vector of numbers.  \n",
        "\n",
        "In this part of Week 2's graded assignment, you will implement the Skip-gram word embedding model for a given set of documents (sentences in our case). You will also observe the trends and similarities between the vector representations of different words.\n",
        "\n",
        "### **Note:**\n",
        "**You are to only write/modify the code in between consecutive `# <START>` and `# <END>` comments. DO NOT modify other parts of the notebook, your assignments will not be graded otherwise.**\n",
        "\n",
        "```python\n",
        "\"Don't modify any code here\"\n",
        "\n",
        "# < START >\n",
        "\"YOUR CODE GOES HERE!\"\n",
        "# < END >\n",
        "\n",
        "\"Don't modify any code here\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lHoIAWcfiUpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import the Libraries**\n",
        "\n",
        "Run the cell below to import all the necessary libraries for training and using the skip-gram model.\n",
        "\n",
        "Some of the important ones\n",
        "- [PyTorch](https://pytorch.org/docs/stable/index.html) for tensors and training the neural network\n",
        "- NLTK (Natural Language ToolKit) for text pre-processing (specifically, stop-word removal and tokenization).\n",
        "- `re` module for RegEx\n",
        "- [Pandas](https://pandas.pydata.org/docs/reference/index.html) for data manipulation\n",
        "- scikit-learn for some implementations of involved matrix maths"
      ],
      "metadata": {
        "id": "tTJTiyRTm6kT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv4BZTQcVZ8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87b3cea-3892-4fa3-8177-132b3da6261a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(10)\n",
        "torch.set_default_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# NLTK (Natural Language ToolKit)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import decomposition\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Graphing\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10,8)\n",
        "\n",
        "# Disable warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**\n",
        "Before you train your model on a body of text, it has to be split into individual _\"words\"_, that have been reduced to their simplest form. This is done in pre-processing, and involves the steps shows below:\n",
        "![picture](https://d2mk45aasx86xg.cloudfront.net/Natural_language_processing_pipeline_e3608ff95c.webp)\n",
        "\n",
        "**In this section, you will implement a simplified pre-processing pipeline for your body of text.** We will not be dealing with the last two steps (dependency parsing, POS tagging) in this assignment."
      ],
      "metadata": {
        "id": "YOa9U7Fo9Nwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Corpus and Sentence Segmentation**\n",
        "According to Google,\n",
        ">**Corpus**  (*noun*)  \n",
        "a collection of written or spoken material in machine-readable form, assembled for the purpose of linguistic research.\n",
        "\n",
        "Simply, a corpus is a collection of text used to train a neural network. Usually the text you have is a long string that you would need to split into individual sentences, but for this assignment this step has already been done.  \n",
        "Run this cell to intialize `corpus`.\n",
        "\n"
      ],
      "metadata": {
        "id": "UPEc3hR5_PPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'Drink milk',\n",
        "    'Drink cold water',\n",
        "    'Drink cold cola',\n",
        "    'He is drinking juice',\n",
        "    'Drinking cola and juice',\n",
        "    'Eat roti for lunch',\n",
        "    'Eat mango',\n",
        "    'Eating a cherry',\n",
        "    'Eating an apple',\n",
        "    'Juice with sugar',\n",
        "    'Cola with sugar',\n",
        "    'Mango is a fruit',\n",
        "    'Apple is a fruit',\n",
        "    'Cherry is a fruit',\n",
        "    'Berlin is in Germany',\n",
        "    'Boston is in USA',\n",
        "    'Mercedes is a car',\n",
        "    'Mercedes is from Germany',\n",
        "    'Ford is a car',\n",
        "    'Ford is a car from USA',\n",
        "]"
      ],
      "metadata": {
        "id": "phreYNlJm4W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBG1JLFg1raJ",
        "outputId": "658c478b-6389-470e-a3aa-bc1ee18a4251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drink cold cola\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**\n",
        "Tokenization is done to break down the text into smaller, more manageable units. This can be achieved by simply breaking them down into individual words, although most tokenizers are a bit more complex.\n",
        "\n",
        "### **Stemming**\n",
        "Stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The process of stemming is used to normalize text and make it easier to process.  This is usually done by removing prefixes and suffixes added to the word.  \n",
        "These algorithms are usually much simpler and faster than **lemmetization**, but don't always produce accurate results, as we will see soon.\n",
        "\n",
        "### **Removing stop words**\n",
        "Stop words are common words in English that do not contribute any real meaning to the sentence. Removing these stop words speeds up processing and allows the application to focus on the more important words instead."
      ],
      "metadata": {
        "id": "VxHyaQ_LI8Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some of the stopwords\n",
        "print(\" \".join(stopwords.words('english')[:20]),\" \".join(stopwords.words('english')[45:60]))\n",
        "\n",
        "# using spacy\n",
        "STOP_WORDS\n"
      ],
      "metadata": {
        "id": "ChGc_GWkRFad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874d4c45-1652-46f3-c3e6-3cf4202caa2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his is are was were be been being have has had having do does did doing\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'ve\",\n",
              " 'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anyhow',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anywhere',\n",
              " 'are',\n",
              " 'around',\n",
              " 'as',\n",
              " 'at',\n",
              " 'back',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'but',\n",
              " 'by',\n",
              " 'ca',\n",
              " 'call',\n",
              " 'can',\n",
              " 'cannot',\n",
              " 'could',\n",
              " 'did',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'done',\n",
              " 'down',\n",
              " 'due',\n",
              " 'during',\n",
              " 'each',\n",
              " 'eight',\n",
              " 'either',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'empty',\n",
              " 'enough',\n",
              " 'even',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'except',\n",
              " 'few',\n",
              " 'fifteen',\n",
              " 'fifty',\n",
              " 'first',\n",
              " 'five',\n",
              " 'for',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forty',\n",
              " 'four',\n",
              " 'from',\n",
              " 'front',\n",
              " 'full',\n",
              " 'further',\n",
              " 'get',\n",
              " 'give',\n",
              " 'go',\n",
              " 'had',\n",
              " 'has',\n",
              " 'have',\n",
              " 'he',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'however',\n",
              " 'hundred',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'indeed',\n",
              " 'into',\n",
              " 'is',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'keep',\n",
              " 'last',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'least',\n",
              " 'less',\n",
              " 'made',\n",
              " 'make',\n",
              " 'many',\n",
              " 'may',\n",
              " 'me',\n",
              " 'meanwhile',\n",
              " 'might',\n",
              " 'mine',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'much',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " \"n't\",\n",
              " 'name',\n",
              " 'namely',\n",
              " 'neither',\n",
              " 'never',\n",
              " 'nevertheless',\n",
              " 'next',\n",
              " 'nine',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'none',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'nothing',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'n‘t',\n",
              " 'n’t',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'or',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 'part',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'please',\n",
              " 'put',\n",
              " 'quite',\n",
              " 'rather',\n",
              " 're',\n",
              " 'really',\n",
              " 'regarding',\n",
              " 'same',\n",
              " 'say',\n",
              " 'see',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'serious',\n",
              " 'several',\n",
              " 'she',\n",
              " 'should',\n",
              " 'show',\n",
              " 'side',\n",
              " 'since',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhere',\n",
              " 'still',\n",
              " 'such',\n",
              " 'take',\n",
              " 'ten',\n",
              " 'than',\n",
              " 'that',\n",
              " 'the',\n",
              " 'their',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'thence',\n",
              " 'there',\n",
              " 'thereafter',\n",
              " 'thereby',\n",
              " 'therefore',\n",
              " 'therein',\n",
              " 'thereupon',\n",
              " 'these',\n",
              " 'they',\n",
              " 'third',\n",
              " 'this',\n",
              " 'those',\n",
              " 'though',\n",
              " 'three',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'thru',\n",
              " 'thus',\n",
              " 'to',\n",
              " 'together',\n",
              " 'too',\n",
              " 'top',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'twelve',\n",
              " 'twenty',\n",
              " 'two',\n",
              " 'under',\n",
              " 'unless',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'us',\n",
              " 'used',\n",
              " 'using',\n",
              " 'various',\n",
              " 'very',\n",
              " 'via',\n",
              " 'was',\n",
              " 'we',\n",
              " 'well',\n",
              " 'were',\n",
              " 'what',\n",
              " 'whatever',\n",
              " 'when',\n",
              " 'whence',\n",
              " 'whenever',\n",
              " 'where',\n",
              " 'whereafter',\n",
              " 'whereas',\n",
              " 'whereby',\n",
              " 'wherein',\n",
              " 'whereupon',\n",
              " 'wherever',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'while',\n",
              " 'whither',\n",
              " 'who',\n",
              " 'whoever',\n",
              " 'whole',\n",
              " 'whom',\n",
              " 'whose',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'would',\n",
              " 'yet',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " '‘d',\n",
              " '‘ll',\n",
              " '‘m',\n",
              " '‘re',\n",
              " '‘s',\n",
              " '‘ve',\n",
              " '’d',\n",
              " '’ll',\n",
              " '’m',\n",
              " '’re',\n",
              " '’s',\n",
              " '’ve'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your corpus has been chosen in such a way that it is mostly cleaned up, but still contains:\n",
        "- a few verbs in their -ing form\n",
        "- a significant amount of stop-words\n",
        "\n",
        "Write a `preprocess` function that takes the corpus as an argument, and outputs a list of lists, where **each inner list representing the corresponing sentence in tokenized form (using NLTK)**, and with:\n",
        "- **the stop words removed**\n",
        "- **\"-ing\" form verbs reduced to their normal forms**\n",
        "\n",
        "using string methods and RegEx (you are not obligated to use both though, feel free to implement it with just one).\n",
        "\n",
        "_Note: **We do not want you to implement a stemmer that converts all -ing form verbs in English to the correct root verb.** Observe that the given corpus only contains verbs that have been converted to their continuous form simply by suffixing \"ing\"_\n",
        "<details>\n",
        "  <summary>Hint (reg NLTK)</summary>\n",
        "The only function you really need from NLTK is <a href=\"https://www.nltk.org/api/nltk.tokenize.word_tokenize.html\"><code>nltk.tokenize.word_tokenize()</code></a>\n",
        "</details>\n",
        "<details>\n",
        "  <summary>Why has the stop words list been converted to a set?</summary>\n",
        "When using the <code>in</code> operator to check for the existence of an element in a list, Python traverses the list checking each element one-by-one. As such, the larger the list the longer it takes.<br>  \n",
        "However, sets are unordered collections implemented using hash maps. What this means is that to check if a set has a certain value, all Python has to do is pass that value in a \"function\" that outputs the location at which the set would store that value if it did contain it. As such, no matter how big the set is, it takes the same amount of time to check for existence.\n",
        "</details>"
      ],
      "metadata": {
        "id": "JzVCHfqURK0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The list of stopwords has been coverted into a set, making it faster to check if a given word is in it\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(corpus):\n",
        "    result = []\n",
        "    for sentence in corpus:\n",
        "      # <START>\n",
        "      # tokenization and lowercasing\n",
        "      words = nltk.tokenize.word_tokenize(sentence)\n",
        "      words = [word.lower() for word in words]\n",
        "\n",
        "      out = []\n",
        "      for word in words:\n",
        "        if word.endswith(\"ing\"):\n",
        "          word = word[:-3]\n",
        "        if word not in stop_words and not word.endswith(\"ing\"):\n",
        "          out.append(word)\n",
        "\n",
        "\n",
        "      # <END>\n",
        "      result.append(out)\n",
        "    return result\n",
        "\n",
        "# Test for if the function behaves as expected, notice the last test case and expected output\n",
        "test_preprocess = preprocess(['Python is a language','The cake is a lie','He is doing practice','Writing code'])\n",
        "#test_preprocess\n",
        "assert test_preprocess == [['python', 'language'],['cake', 'lie'],['practice'],['writ', 'code']]"
      ],
      "metadata": {
        "id": "R24rToagnKWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the assertion test, the extremely simple stemmer is not going to be able to account for different grammar rules. The average stemmer is many times more intricate, but still often generates non-sense words.\n",
        "\n",
        "The common alternative to this is to use **lemmetization**, which is a slower but more pedantic method that always produces sensible words, but may not reduce words to the same initial root word if they have become too different.\n",
        "\n",
        "Lemmetization and stemming are often used together, to complement each other.\n",
        "\n",
        "Now use the `preprocess` function on the given corpus."
      ],
      "metadata": {
        "id": "zVVDQ7xNQGNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = preprocess(corpus)\n",
        "corpus"
      ],
      "metadata": {
        "id": "3c-_9LtPQE5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ec9475-fd99-4ee0-9c79-83fbc7d8f56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['drink', 'milk'],\n",
              " ['drink', 'cold', 'water'],\n",
              " ['drink', 'cold', 'cola'],\n",
              " ['drink', 'juice'],\n",
              " ['drink', 'cola', 'juice'],\n",
              " ['eat', 'roti', 'lunch'],\n",
              " ['eat', 'mango'],\n",
              " ['eat', 'cherry'],\n",
              " ['eat', 'apple'],\n",
              " ['juice', 'sugar'],\n",
              " ['cola', 'sugar'],\n",
              " ['mango', 'fruit'],\n",
              " ['apple', 'fruit'],\n",
              " ['cherry', 'fruit'],\n",
              " ['berlin', 'germany'],\n",
              " ['boston', 'usa'],\n",
              " ['mercedes', 'car'],\n",
              " ['mercedes', 'germany'],\n",
              " ['ford', 'car'],\n",
              " ['ford', 'car', 'usa']]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building the vocabulary**\n",
        "\n",
        "Before we start representing words as vectors, we must first identify how many **unique** words our corpus contains. Complete the below function to create a dictionary of words `vocabulary`. This will contain all the words from the corpus our neural network will be trained on as keys, and a unique incrementing ID for each word, starting from `0` as the corresponding value.\n",
        "\n",
        "_Remember: Dicts are key-value collections_\n",
        "```python\n",
        " dict = {'key': value}\n",
        " dict['new key'] = new_value\n",
        " ```"
      ],
      "metadata": {
        "id": "cgaTWpgb_4pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(corpus):\n",
        "    # Creates a dictionary with all unique words in corpus with id\n",
        "    vocabulary = {}\n",
        "    id = 0\n",
        "    # <START>\n",
        "    for list_of_words in corpus:\n",
        "      for word in list_of_words:\n",
        "        if word not in vocabulary:\n",
        "          vocabulary[word] = id\n",
        "          id = id +1\n",
        "    # <END>\n",
        "    return vocabulary\n",
        "# Test for create_vocabulary\n",
        "assert(create_vocabulary([['words','are','many'],['many','words','exist'],['are','we','exist']])  == {'words': 0, 'are': 1, 'many': 2, 'exist': 3, 'we': 4})\n",
        "vocabulary = create_vocabulary(corpus)\n",
        "vocabulary"
      ],
      "metadata": {
        "id": "eOqAEmIRA0gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72ac571-7ccf-451f-b26b-b0fea8d5a364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'drink': 0,\n",
              " 'milk': 1,\n",
              " 'cold': 2,\n",
              " 'water': 3,\n",
              " 'cola': 4,\n",
              " 'juice': 5,\n",
              " 'eat': 6,\n",
              " 'roti': 7,\n",
              " 'lunch': 8,\n",
              " 'mango': 9,\n",
              " 'cherry': 10,\n",
              " 'apple': 11,\n",
              " 'sugar': 12,\n",
              " 'fruit': 13,\n",
              " 'berlin': 14,\n",
              " 'germany': 15,\n",
              " 'boston': 16,\n",
              " 'usa': 17,\n",
              " 'mercedes': 18,\n",
              " 'car': 19,\n",
              " 'ford': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating the dataset**\n",
        "\n",
        "Now we analyse each sentence in the corpus and create a dataset that holds each `(word, neighbour)` pair.\n",
        "\n",
        "![Neighbor pairing for bi-gram (n=2)](https://miro.medium.com/v2/resize:fit:552/format:webp/1*jkxbwD55_8M3XBRb1bGm7A.png \"Neighbor pairing for bi-gram (n=2)\")\n",
        ">_The word highlighted in yellow is the source word and the words highlighted in green are its neighboring words._\n",
        "\n",
        "Complete the function `prepare_neighbour_set` that takes the tokenized `corpus` and an optional integer `n_gram`, and returns a Pandas DataFrame `result` that contains two columns, `Input` and `Output`. Input column contains each source word, and Output column contains the neighbours of the source word, within a `n_gram` range on either side.\n",
        "\n",
        "$$\n",
        "\\text{['this', 'is', 'a', 'sentence']  (n_gram = 1)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\newcommand\\T{\\Rule{0pt}{1em}{.3em}}\n",
        "\\begin{array}{|c|c|}\n",
        "\\hline Input & Output \\T \\\\\\hline\n",
        "  this \\T & is \\\\\\hline\n",
        "  is \\T & this \\\\\\hline\n",
        "  is \\T & a \\\\\\hline\n",
        "  a \\T & is \\\\\\hline\n",
        "  a \\T & sentence \\\\\\hline\n",
        "  sentence \\T & a \\\\\\hline\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Remember to account for the corner cases, where the Input word may not have enough neighbours on both sides.\n",
        "\n",
        "<details>\n",
        "  <summary>Hint</summary>\n",
        "  To add a row to the DataFrame, use the following code\n",
        "  <code>result = result.append(row, ignore_index = True)</code>\n",
        "  <br>\n",
        "  The row dataframe should have the same columns as the result dataframe.\n",
        "  <br>\n",
        "  This is a deprecated function, but it is apt for the current scenario so feel free to use it\n",
        "</details>"
      ],
      "metadata": {
        "id": "iUlo7zO6GWYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_neighbour_set(corpus, n_gram = 1):\n",
        "    # Creates a dataset with Input column and Output column for neighboring words.\n",
        "    # The number of neighbors = n_gram*2\n",
        "    columns = ['Input', 'Output']\n",
        "    result = pd.DataFrame(columns = columns)\n",
        "    print(corpus)\n",
        "    for sentence in corpus:\n",
        "      for i,word in enumerate(sentence):\n",
        "        # <START>\n",
        "        neighbours = []\n",
        "        for j in range(1,n_gram+1):\n",
        "           # look back\n",
        "           if(i-j>=0):\n",
        "            neighbours.append(sentence[i-j])\n",
        "\n",
        "           # look forward\n",
        "           if(i+j<len(sentence)):\n",
        "            neighbours.append(sentence[i+j])\n",
        "        for k in neighbours:\n",
        "          result = result.append({'Input': word, 'Output': (k)}, ignore_index=True)\n",
        "\n",
        "\n",
        "        # <END>\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "1YYXTmMpBQZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test to check prepare_neighbour_set. The rows are\n",
        "# sorted to make the assertion order-insensitive\n",
        "test_neighbour = prepare_neighbour_set([['this','is','a','sentence']]).sort_values(by=['Input','Output'])\n",
        "\n",
        "test_neighbour_expected = pd.DataFrame({\n",
        "    'Input': ['this','is','is','a','a','sentence'],\n",
        "    'Output': ['is','this','a','is','sentence','a']\n",
        "}).sort_values(by=['Input','Output'])\n",
        "\n",
        "assert test_neighbour.equals(test_neighbour_expected)\n",
        "\n",
        "# Initialize neighbour words dataset for corpus\n",
        "train_emb = prepare_neighbour_set(corpus, n_gram = 2)\n",
        "train_emb.head()"
      ],
      "metadata": {
        "id": "n5zNwck34aJS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "afd7ab1f-9c18-4d27-a290-d7e967fe8b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['this', 'is', 'a', 'sentence']]\n",
            "[['drink', 'milk'], ['drink', 'cold', 'water'], ['drink', 'cold', 'cola'], ['drink', 'juice'], ['drink', 'cola', 'juice'], ['eat', 'roti', 'lunch'], ['eat', 'mango'], ['eat', 'cherry'], ['eat', 'apple'], ['juice', 'sugar'], ['cola', 'sugar'], ['mango', 'fruit'], ['apple', 'fruit'], ['cherry', 'fruit'], ['berlin', 'germany'], ['boston', 'usa'], ['mercedes', 'car'], ['mercedes', 'germany'], ['ford', 'car'], ['ford', 'car', 'usa']]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Input Output\n",
              "0  drink   milk\n",
              "1   milk  drink\n",
              "2  drink   cold\n",
              "3  drink  water\n",
              "4   cold  drink"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-543bc99d-e103-4f5d-963a-251d30d1ed8f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>drink</td>\n",
              "      <td>milk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>milk</td>\n",
              "      <td>drink</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>drink</td>\n",
              "      <td>cold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>drink</td>\n",
              "      <td>water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cold</td>\n",
              "      <td>drink</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-543bc99d-e103-4f5d-963a-251d30d1ed8f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ee39df78-de58-40b2-b916-52d79e7ba52e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee39df78-de58-40b2-b916-52d79e7ba52e')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ee39df78-de58-40b2-b916-52d79e7ba52e button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-543bc99d-e103-4f5d-963a-251d30d1ed8f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-543bc99d-e103-4f5d-963a-251d30d1ed8f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further proceed towards their numerical representations, we shall also replace the words in the data frame with their equivalent IDs in `vocabulary`."
      ],
      "metadata": {
        "id": "axJv9MJscs1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_emb.Input = train_emb.Input.map(vocabulary)\n",
        "train_emb.Output = train_emb.Output.map(vocabulary)\n",
        "train_emb.head(15)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "co3QBNxRnPbN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "3ac4b2fb-bb90-463a-fd17-6c11e9ae88bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Input  Output\n",
              "0       0       1\n",
              "1       1       0\n",
              "2       0       2\n",
              "3       0       3\n",
              "4       2       0\n",
              "5       2       3\n",
              "6       3       2\n",
              "7       3       0\n",
              "8       0       2\n",
              "9       0       4\n",
              "10      2       0\n",
              "11      2       4\n",
              "12      4       2\n",
              "13      4       0\n",
              "14      0       5"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-c9635ba7-58ae-4015-85e5-ea76e3212c75\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9635ba7-58ae-4015-85e5-ea76e3212c75')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-c6efd7e6-d8e4-46a4-8f6e-c924f4b68ff7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c6efd7e6-d8e4-46a4-8f6e-c924f4b68ff7')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-c6efd7e6-d8e4-46a4-8f6e-c924f4b68ff7 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c9635ba7-58ae-4015-85e5-ea76e3212c75 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c9635ba7-58ae-4015-85e5-ea76e3212c75');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**\n",
        "We now begin implementing a neural network to train based on the skip-gram model. For this purpose, we first have to represent the words/tokens we have as vectors.\n",
        "\n",
        "This might sound counter-intuitive since the very reason we are training this model is to find a way to represent words as vectors, but to start we have to try and represent them as vectors as best as possible in the current stage.\n",
        "\n",
        "All we know however at this stage, after all our pre-processing, is that **if there are two different words, they are not the same word** 🤯.  \n",
        "As silly as it sounds, this is the simplest way to represent two words - as vectors that share no similarity with each others.\n",
        "\n",
        "We do this by representing the N words **as unit vectors of a N-dimensional space**.\n",
        "\n",
        "In simpler terms, if we have $N$ words in our vocabulary, we represent them using a vector of length $N$ (technically $1 \\times N$). All of the values of the vector are set to zero, except for one that is set to one. Each word has its \"hot\" bit at a different position, that we can determine using its unique index in `vocabulary`.\n",
        "\n",
        "#### This is where the name **one-hot encoding** comes.\n",
        "\n",
        "![Example of a vector with a vocabulary size of 3](https://miro.medium.com/v2/resize:fit:837/1*d5-PQyRRjvzBZjI5f7X3hA.png \"Example of a vector with a vocabulary size of 3\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lVd-iGNK9UpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement one-hot encoding for the `vocabulary`, with two separate functions:\n",
        "\n",
        "- `get_one_hot` takes an index, and returns a 2-D vector of size $1 \\times$ `vocab_size` corresponding to the index.\n",
        "\n",
        "- `get_input_tensor` takes an 1-D tensor of indexes as input, and returns a 2-D vector of size `batch_size` $\\times$ `vocab_size` (`batch_size` being the length of the input tensor).\n",
        "\n",
        "Both functions also take `vocab_size` as the second argument.\n",
        "> _**Note:** The purpose of `get_one_hot` is to simplify the implementation of `get_input_tensor`. If you have a different method for directly implementing the latter with using the former, feel free to do so._"
      ],
      "metadata": {
        "id": "E_cf7WHTb-nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_one_hot(index, vocab_size):\n",
        "    # <START>\n",
        "    # Create one-hot vector of size vocab_size from index\n",
        "    one_hot = torch.zeros(1, vocab_size)\n",
        "    one_hot[0, index] = 1\n",
        "     # <END>\n",
        "    return one_hot.float()\n",
        "\n",
        "def get_input_tensor(tensor, vocab_size):\n",
        "    batch_size = tensor.shape[0]\n",
        "    # <START>\n",
        "    # Transform 1D tensor of word indexes to one-hot encoded 2D tensor of dimensions (batch_size, vocab_size)\n",
        "    inp = torch.zeros(batch_size, vocab_size)\n",
        "    inp[torch.arange(batch_size), tensor] = 1\n",
        "    # <END>\n",
        "    return inp.float()\n",
        "\n",
        "assert torch.all(get_one_hot(0,3).eq(torch.tensor([[1,0,0]])))\n",
        "assert torch.all(get_input_tensor(torch.tensor([1,2,0]),3).eq(torch.tensor([[0,1,0],[0,0,1],[1,0,0]])))"
      ],
      "metadata": {
        "id": "YTmCY7C3nSir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now initialize the neural network model using PyTorch.\n",
        "\n",
        "We will train the neural network to take the source word (the word in the `Input` column) and will _\"expect\"_ it to predict its neighbors (the word in the `Output` column). Since we pass the source word into the array as its one-hot vector, we similarily will want an output in the form of a vector of the same dimensions as the one-hot vectors.\n",
        "\n",
        "Our final goal, however, is not just creating a neighbor prediction model. It is to find a representation of the words in a vector form **that represents how it is used in context with other words in the vocabulary** - to go from the one-hot representation in `vocab_size` dimensions to a lower dimension (that we can call `embed_dims`) where the words are no longer \"completely dissimilar\".\n",
        "\n",
        "The neural network should consist of two layers:\n",
        "- First, a linear layer of `embed_dims` neurons, with **no activation function** that takes the input one-hot vector\n",
        "- Then, another linear layer of `vocab_size` neurons that predicts the neighboring words\n",
        "\n",
        "![image](https://1.bp.blogspot.com/-Kf0O7V74uHI/XV0H1xlsyWI/AAAAAAAAB08/Lvkkqq1DjKIVjS3-tgbf_7D_Ijad2bBDACLcBGAs/s1600/image005.png)\n",
        "> _An example of one hot implemented with a `vocab_size` of 5, and `embed_dims` of 3_\n",
        "\n",
        "Although the output should represent the probability of what the neighboring words could be for a given source word - in which case usually a softmax activation is applied to the output layer, you will **not be activating the second layer either.**  \n",
        "**Both linear layers should also not have any biases (ie. only weights).**\n",
        "\n",
        "**Taking `embed_dims` as 4, implement the skip-gram model below:**\n",
        "<details>\n",
        "  <summary>Why are we excluding the soft max activation, and how will the model produce proper results without it?</summary>\n",
        "  <br>\n",
        "The softmax function is used to convert model outputs into probabilities by spreading out the probability mass among context words. It assigns high probabilities to likely context words and distributes the remaining probability among other words. This spreading is beneficial for larger datasets with diverse word relationships.\n",
        "<br><br>\n",
        "However, <b>in the case of a small dataset with limited co-occurrence patterns</b>, the softmax function may lead to more diffuse predictions. This diffusion can make it <b>harder to obtain distinct word embeddings and could potentially blur the clustering results</b>.\n",
        "<br><br>\n",
        "By excluding the softmax function, the predictions might become more concentrated and capture the limited co-occurrence patterns more accurately, which can be advantageous for certain tasks like clustering. Although the <b>model does not provide explicit probabilities</b>, it can still learn <b>meaningful representations that capture the similarities and relationships</b> between words based on the co-occurrence information.\n",
        "</details>"
      ],
      "metadata": {
        "id": "sh5QLK1n-1ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocabulary)\n",
        "embed_dims = 4\n",
        "\n",
        "skipgram_model = nn.Sequential(\n",
        "    # <START>\n",
        "    # First linear layer to reduce one-hot input to lower-dimensional vector representation\n",
        "    nn.Linear(vocab_size, embed_dims, bias=False),\n",
        "    # Second linear layer to predict neighboring words\n",
        "    nn.Linear(embed_dims, vocab_size, bias=False)\n",
        "    # <END>\n",
        ")\n",
        "\n",
        "# Check dimensions (will except if not Linear layers)\n",
        "assert skipgram_model[0].weight.shape == (embed_dims,vocab_size)\n",
        "assert skipgram_model[1].weight.shape == (vocab_size,embed_dims)\n",
        "# No bias\n",
        "assert skipgram_model[0].bias == None and skipgram_model[1].bias == None"
      ],
      "metadata": {
        "id": "R-CycFghnfC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, run this cell to initialize the hyper-parameters.\n",
        "- `num_epochs` is the number of iterations, or to be more precise, the number of times we want to go over all the training data\n",
        "- `learning_rate` is the same as before"
      ],
      "metadata": {
        "id": "lmz1rnW4rinR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2000\n",
        "learning_rate = 2e-1"
      ],
      "metadata": {
        "id": "s2TlqPI1nf3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this time, we are segregating into multiple possible outcomes instead of just two (binary classification), we must also use a different loss function.\n",
        "\n",
        "Here, [Cross-Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (a more general version of the previous BCE Loss) is used. Put simply, the loss function now expects the tensor with the predictions outputted by the model, and a **1D tensor** containing **the indexes corresponding to the expected classes (ie. words)**.\n",
        "\n",
        "Keeping this in mind, initialize `train_data` and `train_labels` for the model. Then write the training loop.\n",
        "\n",
        "<details>\n",
        "  <summary>Hint</summary>\n",
        "Some functions you wrote earlier in this assignment might come very handy now.\n",
        "</details>\n",
        "\n",
        "_**Note:** Since the model is expected to predict more that one word as the correct prediction for a given source word, the model can never become \"accurate\". The loss function will drop quickly, but will begin to plateau at a much larger value than what you would have seen last time. You can visualize this in the graph of the loss function plotted after training._\n"
      ],
      "metadata": {
        "id": "J6q4A7t-t_Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss() # define loss func\n",
        "loss_hist = [] # list to store loss values, to plot at the end\n",
        "\n",
        "# <START>\n",
        "# Initialize data and labels\n",
        "train_data = get_input_tensor(torch.tensor(train_emb.Input.values), vocab_size) # Should be a one-hot encoded 2D tensor\n",
        "train_labels = torch.tensor(train_emb.Output.values) #Should be a 1D tensor of the indexes of the neighbor words expected\n",
        "# <END>\n",
        "\n",
        "assert train_data.shape == (train_emb.shape[0], vocab_size)\n",
        "assert train_labels.shape == (train_emb.shape[0],)\n",
        "\n",
        "optimizer = optim.SGD(skipgram_model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 70000\n",
        "loss_hist = []  # list to store loss values, to plot at the end\n",
        "\n",
        "for epo in range(num_epochs):\n",
        "    # Forward propagation: Compute predictions\n",
        "    predictions = skipgram_model(train_data)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_fn(predictions, train_labels)\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backpropagation step\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights using gradient descent.\n",
        "    optimizer.step()\n",
        "        # <END>\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "\n",
        "    # Print the loss every 250 epochs\n",
        "    if epo%250 == 0:\n",
        "      print(f'Epoch {epo}, loss = {loss}')\n",
        "\n",
        "plt.plot(loss_hist)\n",
        "plt.xlabel(\"No. of epochs\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "iGR4aeyGnipH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d683c2e1-04dd-4b17-cdad-90138547c0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss = 1.2984522581100464\n",
            "Epoch 250, loss = 1.2867416143417358\n",
            "Epoch 500, loss = 1.2757933139801025\n",
            "Epoch 750, loss = 1.265550971031189\n",
            "Epoch 1000, loss = 1.2559617757797241\n",
            "Epoch 1250, loss = 1.246976613998413\n",
            "Epoch 1500, loss = 1.2385499477386475\n",
            "Epoch 1750, loss = 1.2306398153305054\n",
            "Epoch 2000, loss = 1.2232067584991455\n",
            "Epoch 2250, loss = 1.2162151336669922\n",
            "Epoch 2500, loss = 1.209631323814392\n",
            "Epoch 2750, loss = 1.2034246921539307\n",
            "Epoch 3000, loss = 1.1975674629211426\n",
            "Epoch 3250, loss = 1.192033052444458\n",
            "Epoch 3500, loss = 1.1867973804473877\n",
            "Epoch 3750, loss = 1.1818389892578125\n",
            "Epoch 4000, loss = 1.1771377325057983\n",
            "Epoch 4250, loss = 1.1726747751235962\n",
            "Epoch 4500, loss = 1.168433427810669\n",
            "Epoch 4750, loss = 1.164397954940796\n",
            "Epoch 5000, loss = 1.160554051399231\n",
            "Epoch 5250, loss = 1.156888723373413\n",
            "Epoch 5500, loss = 1.153389811515808\n",
            "Epoch 5750, loss = 1.1500465869903564\n",
            "Epoch 6000, loss = 1.146848201751709\n",
            "Epoch 6250, loss = 1.1437855958938599\n",
            "Epoch 6500, loss = 1.1408501863479614\n",
            "Epoch 6750, loss = 1.1380337476730347\n",
            "Epoch 7000, loss = 1.1353286504745483\n",
            "Epoch 7250, loss = 1.132728099822998\n",
            "Epoch 7500, loss = 1.130225658416748\n",
            "Epoch 7750, loss = 1.1278150081634521\n",
            "Epoch 8000, loss = 1.125490427017212\n",
            "Epoch 8250, loss = 1.1232469081878662\n",
            "Epoch 8500, loss = 1.1210790872573853\n",
            "Epoch 8750, loss = 1.1189823150634766\n",
            "Epoch 9000, loss = 1.1169523000717163\n",
            "Epoch 9250, loss = 1.1149845123291016\n",
            "Epoch 9500, loss = 1.1130752563476562\n",
            "Epoch 9750, loss = 1.1112208366394043\n",
            "Epoch 10000, loss = 1.1094175577163696\n",
            "Epoch 10250, loss = 1.1076622009277344\n",
            "Epoch 10500, loss = 1.1059516668319702\n",
            "Epoch 10750, loss = 1.1042829751968384\n",
            "Epoch 11000, loss = 1.1026535034179688\n",
            "Epoch 11250, loss = 1.1010605096817017\n",
            "Epoch 11500, loss = 1.0995014905929565\n",
            "Epoch 11750, loss = 1.0979745388031006\n",
            "Epoch 12000, loss = 1.0964772701263428\n",
            "Epoch 12250, loss = 1.0950076580047607\n",
            "Epoch 12500, loss = 1.0935641527175903\n",
            "Epoch 12750, loss = 1.0921452045440674\n",
            "Epoch 13000, loss = 1.090748906135559\n",
            "Epoch 13250, loss = 1.0893741846084595\n",
            "Epoch 13500, loss = 1.088019847869873\n",
            "Epoch 13750, loss = 1.0866849422454834\n",
            "Epoch 14000, loss = 1.0853685140609741\n",
            "Epoch 14250, loss = 1.0840693712234497\n",
            "Epoch 14500, loss = 1.0827876329421997\n",
            "Epoch 14750, loss = 1.0815221071243286\n",
            "Epoch 15000, loss = 1.0802727937698364\n",
            "Epoch 15250, loss = 1.0790398120880127\n",
            "Epoch 15500, loss = 1.077822208404541\n",
            "Epoch 15750, loss = 1.0766205787658691\n",
            "Epoch 16000, loss = 1.075434923171997\n",
            "Epoch 16250, loss = 1.0742648839950562\n",
            "Epoch 16500, loss = 1.0731111764907837\n",
            "Epoch 16750, loss = 1.0719739198684692\n",
            "Epoch 17000, loss = 1.0708534717559814\n",
            "Epoch 17250, loss = 1.0697498321533203\n",
            "Epoch 17500, loss = 1.0686638355255127\n",
            "Epoch 17750, loss = 1.0675956010818481\n",
            "Epoch 18000, loss = 1.0665457248687744\n",
            "Epoch 18250, loss = 1.065514087677002\n",
            "Epoch 18500, loss = 1.0645016431808472\n",
            "Epoch 18750, loss = 1.06350839138031\n",
            "Epoch 19000, loss = 1.0625344514846802\n",
            "Epoch 19250, loss = 1.0615801811218262\n",
            "Epoch 19500, loss = 1.060645580291748\n",
            "Epoch 19750, loss = 1.059731125831604\n",
            "Epoch 20000, loss = 1.058836817741394\n",
            "Epoch 20250, loss = 1.0579625368118286\n",
            "Epoch 20500, loss = 1.0571081638336182\n",
            "Epoch 20750, loss = 1.0562738180160522\n",
            "Epoch 21000, loss = 1.0554590225219727\n",
            "Epoch 21250, loss = 1.0546637773513794\n",
            "Epoch 21500, loss = 1.053887963294983\n",
            "Epoch 21750, loss = 1.053131103515625\n",
            "Epoch 22000, loss = 1.0523931980133057\n",
            "Epoch 22250, loss = 1.0516735315322876\n",
            "Epoch 22500, loss = 1.0509721040725708\n",
            "Epoch 22750, loss = 1.0502880811691284\n",
            "Epoch 23000, loss = 1.0496214628219604\n",
            "Epoch 23250, loss = 1.0489717721939087\n",
            "Epoch 23500, loss = 1.0483382940292358\n",
            "Epoch 23750, loss = 1.0477211475372314\n",
            "Epoch 24000, loss = 1.0471192598342896\n",
            "Epoch 24250, loss = 1.0465326309204102\n",
            "Epoch 24500, loss = 1.045960545539856\n",
            "Epoch 24750, loss = 1.0454028844833374\n",
            "Epoch 25000, loss = 1.0448589324951172\n",
            "Epoch 25250, loss = 1.0443284511566162\n",
            "Epoch 25500, loss = 1.0438109636306763\n",
            "Epoch 25750, loss = 1.0433059930801392\n",
            "Epoch 26000, loss = 1.0428134202957153\n",
            "Epoch 26250, loss = 1.0423325300216675\n",
            "Epoch 26500, loss = 1.041863203048706\n",
            "Epoch 26750, loss = 1.0414049625396729\n",
            "Epoch 27000, loss = 1.0409572124481201\n",
            "Epoch 27250, loss = 1.040520191192627\n",
            "Epoch 27500, loss = 1.0400928258895874\n",
            "Epoch 27750, loss = 1.0396754741668701\n",
            "Epoch 28000, loss = 1.0392674207687378\n",
            "Epoch 28250, loss = 1.0388685464859009\n",
            "Epoch 28500, loss = 1.0384786128997803\n",
            "Epoch 28750, loss = 1.0380972623825073\n",
            "Epoch 29000, loss = 1.0377241373062134\n",
            "Epoch 29250, loss = 1.0373592376708984\n",
            "Epoch 29500, loss = 1.0370019674301147\n",
            "Epoch 29750, loss = 1.0366523265838623\n",
            "Epoch 30000, loss = 1.036310076713562\n",
            "Epoch 30250, loss = 1.0359750986099243\n",
            "Epoch 30500, loss = 1.0356467962265015\n",
            "Epoch 30750, loss = 1.035325527191162\n",
            "Epoch 31000, loss = 1.035010576248169\n",
            "Epoch 31250, loss = 1.0347020626068115\n",
            "Epoch 31500, loss = 1.0343995094299316\n",
            "Epoch 31750, loss = 1.0341031551361084\n",
            "Epoch 32000, loss = 1.0338128805160522\n",
            "Epoch 32250, loss = 1.0335278511047363\n",
            "Epoch 32500, loss = 1.0332486629486084\n",
            "Epoch 32750, loss = 1.0329747200012207\n",
            "Epoch 33000, loss = 1.0327059030532837\n",
            "Epoch 33250, loss = 1.0324424505233765\n",
            "Epoch 33500, loss = 1.0321840047836304\n",
            "Epoch 33750, loss = 1.0319302082061768\n",
            "Epoch 34000, loss = 1.031680941581726\n",
            "Epoch 34250, loss = 1.0314363241195679\n",
            "Epoch 34500, loss = 1.0311964750289917\n",
            "Epoch 34750, loss = 1.0309607982635498\n",
            "Epoch 35000, loss = 1.030729055404663\n",
            "Epoch 35250, loss = 1.0305019617080688\n",
            "Epoch 35500, loss = 1.0302788019180298\n",
            "Epoch 35750, loss = 1.0300594568252563\n",
            "Epoch 36000, loss = 1.0298441648483276\n",
            "Epoch 36250, loss = 1.0296324491500854\n",
            "Epoch 36500, loss = 1.0294246673583984\n",
            "Epoch 36750, loss = 1.0292203426361084\n",
            "Epoch 37000, loss = 1.0290195941925049\n",
            "Epoch 37250, loss = 1.028822422027588\n",
            "Epoch 37500, loss = 1.0286284685134888\n",
            "Epoch 37750, loss = 1.028437614440918\n",
            "Epoch 38000, loss = 1.0282503366470337\n",
            "Epoch 38250, loss = 1.0280660390853882\n",
            "Epoch 38500, loss = 1.027884840965271\n",
            "Epoch 38750, loss = 1.0277066230773926\n",
            "Epoch 39000, loss = 1.027531385421753\n",
            "Epoch 39250, loss = 1.027359127998352\n",
            "Epoch 39500, loss = 1.0271896123886108\n",
            "Epoch 39750, loss = 1.0270228385925293\n",
            "Epoch 40000, loss = 1.0268588066101074\n",
            "Epoch 40250, loss = 1.0266975164413452\n",
            "Epoch 40500, loss = 1.0265387296676636\n",
            "Epoch 40750, loss = 1.026382565498352\n",
            "Epoch 41000, loss = 1.026228666305542\n",
            "Epoch 41250, loss = 1.026077389717102\n",
            "Epoch 41500, loss = 1.0259286165237427\n",
            "Epoch 41750, loss = 1.0257822275161743\n",
            "Epoch 42000, loss = 1.0256381034851074\n",
            "Epoch 42250, loss = 1.025496006011963\n",
            "Epoch 42500, loss = 1.0253562927246094\n",
            "Epoch 42750, loss = 1.0252187252044678\n",
            "Epoch 43000, loss = 1.0250831842422485\n",
            "Epoch 43250, loss = 1.0249499082565308\n",
            "Epoch 43500, loss = 1.024818778038025\n",
            "Epoch 43750, loss = 1.0246893167495728\n",
            "Epoch 44000, loss = 1.0245620012283325\n",
            "Epoch 44250, loss = 1.0244364738464355\n",
            "Epoch 44500, loss = 1.024312973022461\n",
            "Epoch 44750, loss = 1.0241912603378296\n",
            "Epoch 45000, loss = 1.024071455001831\n",
            "Epoch 45250, loss = 1.0239534378051758\n",
            "Epoch 45500, loss = 1.0238370895385742\n",
            "Epoch 45750, loss = 1.023722529411316\n",
            "Epoch 46000, loss = 1.0236096382141113\n",
            "Epoch 46250, loss = 1.023498296737671\n",
            "Epoch 46500, loss = 1.0233886241912842\n",
            "Epoch 46750, loss = 1.0232806205749512\n",
            "Epoch 47000, loss = 1.0231740474700928\n",
            "Epoch 47250, loss = 1.023069143295288\n",
            "Epoch 47500, loss = 1.022965908050537\n",
            "Epoch 47750, loss = 1.0228638648986816\n",
            "Epoch 48000, loss = 1.0227634906768799\n",
            "Epoch 48250, loss = 1.0226643085479736\n",
            "Epoch 48500, loss = 1.022566556930542\n",
            "Epoch 48750, loss = 1.0224701166152954\n",
            "Epoch 49000, loss = 1.0223753452301025\n",
            "Epoch 49250, loss = 1.0222817659378052\n",
            "Epoch 49500, loss = 1.0221892595291138\n",
            "Epoch 49750, loss = 1.0220983028411865\n",
            "Epoch 50000, loss = 1.0220085382461548\n",
            "Epoch 50250, loss = 1.021919846534729\n",
            "Epoch 50500, loss = 1.0218323469161987\n",
            "Epoch 50750, loss = 1.0217461585998535\n",
            "Epoch 51000, loss = 1.0216612815856934\n",
            "Epoch 51250, loss = 1.0215774774551392\n",
            "Epoch 51500, loss = 1.021494746208191\n",
            "Epoch 51750, loss = 1.021412968635559\n",
            "Epoch 52000, loss = 1.0213325023651123\n",
            "Epoch 52250, loss = 1.0212528705596924\n",
            "Epoch 52500, loss = 1.0211743116378784\n",
            "Epoch 52750, loss = 1.0210967063903809\n",
            "Epoch 53000, loss = 1.0210204124450684\n",
            "Epoch 53250, loss = 1.0209451913833618\n",
            "Epoch 53500, loss = 1.020870566368103\n",
            "Epoch 53750, loss = 1.0207970142364502\n",
            "Epoch 54000, loss = 1.0207245349884033\n",
            "Epoch 54250, loss = 1.0206528902053833\n",
            "Epoch 54500, loss = 1.0205820798873901\n",
            "Epoch 54750, loss = 1.020512342453003\n",
            "Epoch 55000, loss = 1.0204434394836426\n",
            "Epoch 55250, loss = 1.020375370979309\n",
            "Epoch 55500, loss = 1.020308017730713\n",
            "Epoch 55750, loss = 1.020241618156433\n",
            "Epoch 56000, loss = 1.0201761722564697\n",
            "Epoch 56250, loss = 1.020111322402954\n",
            "Epoch 56500, loss = 1.0200473070144653\n",
            "Epoch 56750, loss = 1.0199841260910034\n",
            "Epoch 57000, loss = 1.0199217796325684\n",
            "Epoch 57250, loss = 1.0198599100112915\n",
            "Epoch 57500, loss = 1.019798994064331\n",
            "Epoch 57750, loss = 1.0197389125823975\n",
            "Epoch 58000, loss = 1.0196794271469116\n",
            "Epoch 58250, loss = 1.0196207761764526\n",
            "Epoch 58500, loss = 1.0195626020431519\n",
            "Epoch 58750, loss = 1.019505262374878\n",
            "Epoch 59000, loss = 1.0194485187530518\n",
            "Epoch 59250, loss = 1.019392490386963\n",
            "Epoch 59500, loss = 1.0193371772766113\n",
            "Epoch 59750, loss = 1.0192822217941284\n",
            "Epoch 60000, loss = 1.0192279815673828\n",
            "Epoch 60250, loss = 1.019174575805664\n",
            "Epoch 60500, loss = 1.0191216468811035\n",
            "Epoch 60750, loss = 1.0190691947937012\n",
            "Epoch 61000, loss = 1.0190175771713257\n",
            "Epoch 61250, loss = 1.0189663171768188\n",
            "Epoch 61500, loss = 1.0189156532287598\n",
            "Epoch 61750, loss = 1.0188658237457275\n",
            "Epoch 62000, loss = 1.0188164710998535\n",
            "Epoch 62250, loss = 1.0187675952911377\n",
            "Epoch 62500, loss = 1.0187193155288696\n",
            "Epoch 62750, loss = 1.0186713933944702\n",
            "Epoch 63000, loss = 1.0186240673065186\n",
            "Epoch 63250, loss = 1.0185770988464355\n",
            "Epoch 63500, loss = 1.0185309648513794\n",
            "Epoch 63750, loss = 1.0184850692749023\n",
            "Epoch 64000, loss = 1.0184398889541626\n",
            "Epoch 64250, loss = 1.0183950662612915\n",
            "Epoch 64500, loss = 1.0183507204055786\n",
            "Epoch 64750, loss = 1.0183067321777344\n",
            "Epoch 65000, loss = 1.0182632207870483\n",
            "Epoch 65250, loss = 1.0182204246520996\n",
            "Epoch 65500, loss = 1.01817786693573\n",
            "Epoch 65750, loss = 1.0181357860565186\n",
            "Epoch 66000, loss = 1.0180941820144653\n",
            "Epoch 66250, loss = 1.0180529356002808\n",
            "Epoch 66500, loss = 1.018012285232544\n",
            "Epoch 66750, loss = 1.0179718732833862\n",
            "Epoch 67000, loss = 1.0179320573806763\n",
            "Epoch 67250, loss = 1.0178923606872559\n",
            "Epoch 67500, loss = 1.0178531408309937\n",
            "Epoch 67750, loss = 1.0178143978118896\n",
            "Epoch 68000, loss = 1.0177760124206543\n",
            "Epoch 68250, loss = 1.017737865447998\n",
            "Epoch 68500, loss = 1.017700433731079\n",
            "Epoch 68750, loss = 1.0176631212234497\n",
            "Epoch 69000, loss = 1.0176262855529785\n",
            "Epoch 69250, loss = 1.0175896883010864\n",
            "Epoch 69500, loss = 1.0175535678863525\n",
            "Epoch 69750, loss = 1.0175178050994873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using the Skip-Gram embedding**\n",
        "By training the model to perform this fake task, we have now created a neural network that when given a one hot encoded from the vocabulary, reduces it to a vector of length `embed_dims`, and then tries to predict what words could be its neighbors.\n",
        "\n",
        "What we are interested in, is the first part of this process - the reduction of a 21 long vector to one only 4 long.\n",
        "\n",
        "We shall now extract the parameter responsible for this conversion - the weights of the first layer `W1`.\n",
        "\n",
        "Access the tensor containing the weights of the first hidden layer, and create a \"detached\" copy of it, converted to a Numpy ndarray. Due to how PyTorch stores its weights, you will also have to transpose the matrix."
      ],
      "metadata": {
        "id": "cSsRzZIo9Ytv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <START>\n",
        "W1 = skipgram_model[0].weight.detach().numpy().T\n",
        "# <END>\n",
        "\n",
        "assert isinstance(W1,np.ndarray) and W1.shape == (vocab_size,embed_dims)"
      ],
      "metadata": {
        "id": "V00aF7yZnlwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now have a matrix, that given a one hot word embedding, can map it to another embedding of lesser dimensions. You will see that the word embedding for a given index is very simply just the row of `W1` corresponding to said index.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 0 & 0 &\\cdots & 0\n",
        "\\end{bmatrix}_{1×V}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "w^{1}_{1,1} & w^{1}_{1,2} &\\cdots & w^{1}_{1,N}\\\\\n",
        "w^{1}_{2,1} & w^{1}_{2,2} &\\cdots & w^{1}_{2,N}\\\\\n",
        "\\vdots & \\vdots &\\ddots & \\vdots\\\\\n",
        "w^{1}_{V,1} & w^{1}_{V,2} &\\cdots & w^{1}_{V,N}\\\\\n",
        "\\end{bmatrix}_{V×N}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "w^{1}_{2,1} & w^{1}_{2,2} &\\cdots & w^{1}_{2,N}\\\\\n",
        "\\end{bmatrix}_{1×N}\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "\\text{Here, V is vocab_size and N is embed_dims}\n",
        "$$\n",
        "> _Note: While the sizes remain the same, in Python everything is zero-indexed (first element is zero), not one-indexed like mathematical arrays. What you see here is the computation of the one-hot vector corresponding to index 1 in Python_\n",
        "\n",
        "### **Graphical Representation**\n",
        "\n",
        "We now wish to observe the similarities and relations between the generated word embeddings, and the simplest way to do so is graphically. However, we cannot easily represent a complete 4D vector in a graph.\n",
        "\n",
        "To solve this, we shall use **SVD (Single Value Decomposition)** to reduce the $21\\times4$ matrix to a $21\\times2$ matrix, containing only the **most sailent features** of the matrix.\n",
        "\n",
        "You need not understand this concept however, and all you need to know is that **in exchange for reducing the number of features of our word we care about, we are able to represent the embedding in even lesser dimensions.**  \n",
        "Note however, that it **may not always be** that the **reduction in features** must also lead to **a reduction in quality and usability**, as you shall see.\n",
        "\n",
        "<details>\n",
        "  <summary>Resources on SVD (Optional)</summary>\n",
        "  <ul>\n",
        "  <li><a href = \"http://timbaumann.info/svd-image-compression-demo/\">Short article with demo of usage in image compression</a></li>\n",
        "  <li><a href = \"https://www.youtube.com/watch?v=OvzJiur55vo\">Youtube video on application in NLP</a></li>\n",
        "  </ul>\n",
        "</details>"
      ],
      "metadata": {
        "id": "3IZlE9xvDFyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svd = decomposition.TruncatedSVD(n_components=2)\n",
        "W1_dec = svd.fit_transform(W1) #Now a 21 x 2 matrix\n",
        "\n",
        "# Just like we saw in the original W1, each row is\n",
        "# a word embedding, i.e a given column contains the\n",
        "# components of every word embedding along a certain \"direction\"\n",
        "x = W1_dec[:,0]\n",
        "y = W1_dec[:,1]\n",
        "plot = sns.scatterplot(x=x, y=y)\n",
        "\n",
        "# Red dot on the origin\n",
        "plot.scatter(x=0, y=0, color='r',s=10)\n",
        "\n",
        "for i in range(0,W1_dec.shape[0]):\n",
        "     plot.text(x[i], y[i], list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='normal');\n"
      ],
      "metadata": {
        "id": "IjxurHdfnmrh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "049c42ef-ba58-4027-87e5-f91934146914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see one very closely arranged group of similar words, along with one or more slightly dispersed yet clearly and separately visible groups of similar words (how separate they are could change if you have changed the hyper-parameters).\n",
        "### **Comparing quantitatively using Cosine Similarity**\n",
        "\n",
        "To better understand how closely related any two words our, we shall use a simple metric called **cosine similarity**.\n",
        "\n",
        "Just like in the vector algebra you know, the smaller the angle between two vectors, the larger the value of its cosine will be. Therefore, by finding the cosine of the angle between two word embeddings, we can compare how similar they are.\n",
        "\n",
        "First, complete the function `get_embeddings`, that for a given word, returns the corresponding 4D and 2D word embeddings. `cosine_similarity` will not accept rank one arrays, make sure the array is \"2D\" (in a different sense than the word embeddings)."
      ],
      "metadata": {
        "id": "3HnTYToRSuxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(word):\n",
        "  # <START>\n",
        "  # Get the 4D and 2D representations corresponding to the word\n",
        "  # using vocabulary, W1 and W1_dec\n",
        "  # Find the index of the word in the vocabulary\n",
        "    word_index = vocabulary.get(word, -1)\n",
        "\n",
        "    # Check if the word is in the vocabulary\n",
        "    if word_index != -1:\n",
        "        # Get the 4D representation from W1 (transpose to make it 2D)\n",
        "        vec_4d = W1[word_index].reshape(1, -1)\n",
        "\n",
        "        # Get the 2D representation from W1_dec\n",
        "        vec_2d = W1_dec[word_index].reshape(1, -1)\n",
        "\n",
        "    else:\n",
        "        # If the word is not in the vocabulary, set both vectors to None\n",
        "        vec_4d = None\n",
        "        vec_2d = None\n",
        "\n",
        "    return vec_4d, vec_2d\n"
      ],
      "metadata": {
        "id": "YOFkam1ikdMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try checking out the similarity of words in both 4D and 2D word embeddings."
      ],
      "metadata": {
        "id": "eI-PPb9TdDkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <START>\n",
        "word1 = 'cherry'\n",
        "word2 = 'apple'\n",
        "# <END>\n",
        "\n",
        "assert vocabulary[word1] != None and vocabulary[word2] != None\n",
        "\n",
        "word1_4d, word1_2d = get_embeddings(word1)\n",
        "word2_4d, word2_2d = get_embeddings(word2)\n",
        "\n",
        "print(f'Cosine similarity of {word1} and {word2}:')\n",
        "print('4D: {:.3f}'.format(cosine_similarity(word1_4d,word2_4d)[0,0]))\n",
        "print('2D: {:.3f}'.format(cosine_similarity(word1_2d,word2_2d)[0,0]))"
      ],
      "metadata": {
        "id": "GyNXgH4mVrZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad1924f-4377-4855-daa2-623974d8c85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity of cherry and apple:\n",
            "4D: 1.000\n",
            "2D: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optional**\n",
        "### Please submit the assignment before you start with this section\n",
        "**Then make a different copy of the notebook and continue there**\n",
        "\n",
        "- Write a function that given a word, prints the most similar word to it for both the original and reduced word embeddings.  \n",
        "- Then try and observe what kind of words are next to each other. Does the pattern make sense?\n",
        "- Try adding more sentences to the corpus, making sure to not upset the preprocessor. Ex:\n",
        " - He is drinking milk in Boston\n",
        " - He is drawing a car\n",
        " - She was drawing an apple\n",
        " - A mango in Boston\n",
        " - He is drinking mango juice\n",
        " - He is eating sugar\n",
        "<details>\n",
        " <summary>Verbs that follow the simple -ing suffixation</summary>\n",
        "<ul>\n",
        "  <li>Walking</li>\n",
        "  <li>Singing</li>\n",
        "  <li>Jumping</li>\n",
        "  <li>Reading</li>\n",
        "  <li>Playing</li>\n",
        "  <li>Talking</li>\n",
        "  <li>Cooking</li>\n",
        "  <li>Flying</li>\n",
        "  <li>Studying</li>\n",
        "  <li>Painting</li>\n",
        "  <li>Climbing</li>\n",
        "  <li>Thinking</li>\n",
        "  <li>Sleeping</li>\n",
        "  <li>Listening</li>\n",
        "</ul>\n",
        "</details>\n",
        "- Feel free to play around with hyper-parameters, maybe adding back the softmax function and seeing how this changes the requirements for training the model"
      ],
      "metadata": {
        "id": "sBll4_B6gEeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closest_neighbors(word):\n",
        "  # <START>\n",
        "  pass\n",
        "  # <END>\n"
      ],
      "metadata": {
        "id": "LjsyLocYdSjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in vocabulary.keys():\n",
        "  closest_neighbors(w)"
      ],
      "metadata": {
        "id": "tN_J4U9kfZ-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}